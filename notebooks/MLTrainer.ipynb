{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLTrainer import MLTrainer\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(random_state=42, n_samples=10000, n_features=5, n_classes=3,\n",
    "                          n_informative=2, n_clusters_per_class=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Multiple Models Using MLTrainer\n",
    "\n",
    "Multiple Sklearn models will be trained using either with GridSearchCV or without. The trained models, their parameters and cross validation scores will be stored.\n",
    "\n",
    "Explanations of parameters of methods can be seen in the source code itself. Parameters are fully laid out when calling them in this notebook.\n",
    "\n",
    "Model parameters used for grid search are found in source code, default model parameters are used when grid search is not enabled.\n",
    "\n",
    ".cv_scores attribute contains a Pandas DataFrame containing model names, parameters, mean cross validation scores for each batch and remark. Remark will only have an entry if the model was not able to be trained.\n",
    "\n",
    ".models attribute contains a list of trained model objects\n",
    "\n",
    ".fit method trains multiple models on training set and saves their mean cross validation scores in a Pandas DataFrame\n",
    "\n",
    ".evaluate method generates scores for test set for each model, then saves classification reports, confusion matrices and label probabilities in CSV. Each model will have its own folder in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:28:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[06:28:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[06:28:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[06:28:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[06:28:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[06:28:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  FitFailedWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  FitFailedWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  FitFailedWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  FitFailedWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  FitFailedWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Negative values in data passed to ComplementNB (input X)\n",
      "\n",
      "  FitFailedWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Negative values in data passed to ComplementNB (input X)\n",
      "\n",
      "  FitFailedWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Negative values in data passed to ComplementNB (input X)\n",
      "\n",
      "  FitFailedWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Negative values in data passed to ComplementNB (input X)\n",
      "\n",
      "  FitFailedWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Negative values in data passed to ComplementNB (input X)\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MLTrainer.models.MLTrainer at 0x7f99dbdd0cd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = MLTrainer(ensemble=True, linear=True, naive_bayes=True, neighbors=True, svm=True, decision_tree=True, seed=100)\n",
    "models.fit(X=X_train, Y=y_train, n_folds=5, scoring=\"accuracy\", n_jobs=-1, gridsearchcv=False, param_grids={}, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>parameters</th>\n",
       "      <th>mean_cv_accuracy</th>\n",
       "      <th>remarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "      <td>0.892667</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bagging</td>\n",
       "      <td>{'base_estimator': None, 'bootstrap': True, 'b...</td>\n",
       "      <td>0.943333</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>extratrees</td>\n",
       "      <td>{'bootstrap': False, 'ccp_alpha': 0.0, 'class_...</td>\n",
       "      <td>0.941333</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gradientboosting</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'criterion': 'friedman_mse'...</td>\n",
       "      <td>0.950267</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>randomforest</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...</td>\n",
       "      <td>0.946933</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>{'objective': 'multi:softprob', 'use_label_enc...</td>\n",
       "      <td>0.945467</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logreg</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "      <td>0.938533</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bernoulli</td>\n",
       "      <td>{'alpha': 1.0, 'binarize': 0.0, 'class_prior':...</td>\n",
       "      <td>0.904400</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gaussian</td>\n",
       "      <td>{'priors': None, 'var_smoothing': 1e-09}</td>\n",
       "      <td>0.943733</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>multinomial</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negative values in data passed to MultinomialN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>complement</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negative values in data passed to ComplementNB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'algorithm': 'auto', 'leaf_size': 30, 'metric...</td>\n",
       "      <td>0.945333</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nu</td>\n",
       "      <td>{'break_ties': False, 'cache_size': 200, 'clas...</td>\n",
       "      <td>0.945733</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>svc</td>\n",
       "      <td>{'C': 1.0, 'break_ties': False, 'cache_size': ...</td>\n",
       "      <td>0.950800</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>decision</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'crit...</td>\n",
       "      <td>0.926933</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>extra</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'crit...</td>\n",
       "      <td>0.923867</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model                                         parameters  \\\n",
       "0           adaboost  {'algorithm': 'SAMME.R', 'base_estimator': Non...   \n",
       "1            bagging  {'base_estimator': None, 'bootstrap': True, 'b...   \n",
       "2         extratrees  {'bootstrap': False, 'ccp_alpha': 0.0, 'class_...   \n",
       "3   gradientboosting  {'ccp_alpha': 0.0, 'criterion': 'friedman_mse'...   \n",
       "4       randomforest  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...   \n",
       "5            xgboost  {'objective': 'multi:softprob', 'use_label_enc...   \n",
       "6             logreg  {'C': 1.0, 'class_weight': None, 'dual': False...   \n",
       "7          bernoulli  {'alpha': 1.0, 'binarize': 0.0, 'class_prior':...   \n",
       "8           gaussian           {'priors': None, 'var_smoothing': 1e-09}   \n",
       "9        multinomial                                               None   \n",
       "10        complement                                               None   \n",
       "11               knn  {'algorithm': 'auto', 'leaf_size': 30, 'metric...   \n",
       "12                nu  {'break_ties': False, 'cache_size': 200, 'clas...   \n",
       "13               svc  {'C': 1.0, 'break_ties': False, 'cache_size': ...   \n",
       "14          decision  {'ccp_alpha': 0.0, 'class_weight': None, 'crit...   \n",
       "15             extra  {'ccp_alpha': 0.0, 'class_weight': None, 'crit...   \n",
       "\n",
       "    mean_cv_accuracy                                            remarks  \n",
       "0           0.892667                                                     \n",
       "1           0.943333                                                     \n",
       "2           0.941333                                                     \n",
       "3           0.950267                                                     \n",
       "4           0.946933                                                     \n",
       "5           0.945467                                                     \n",
       "6           0.938533                                                     \n",
       "7           0.904400                                                     \n",
       "8           0.943733                                                     \n",
       "9                NaN  Negative values in data passed to MultinomialN...  \n",
       "10               NaN  Negative values in data passed to ComplementNB...  \n",
       "11          0.945333                                                     \n",
       "12          0.945733                                                     \n",
       "13          0.950800                                                     \n",
       "14          0.926933                                                     \n",
       "15          0.923867                                                     "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=100),\n",
       " BaggingClassifier(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
       "                   max_features=1.0, max_samples=1.0, n_estimators=10, n_jobs=-1,\n",
       "                   oob_score=False, random_state=100, verbose=0,\n",
       "                   warm_start=False),\n",
       " ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                      criterion='gini', max_depth=None, max_features='auto',\n",
       "                      max_leaf_nodes=None, max_samples=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "                      oob_score=False, random_state=100, verbose=0,\n",
       "                      warm_start=False),\n",
       " GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                            learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                            max_features=None, max_leaf_nodes=None,\n",
       "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                            n_iter_no_change=None, presort='deprecated',\n",
       "                            random_state=100, subsample=1.0, tol=0.0001,\n",
       "                            validation_fraction=0.1, verbose=0,\n",
       "                            warm_start=False),\n",
       " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=-1, oob_score=False, random_state=100, verbose=0,\n",
       "                        warm_start=False),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "               importance_type='gain', interaction_constraints='',\n",
       "               learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "               n_estimators=100, n_jobs=-1, num_parallel_tree=1,\n",
       "               objective='multi:softprob', random_state=100, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "               tree_method='exact', use_label_encoder=True,\n",
       "               validate_parameters=1, verbosity=None),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                    random_state=100, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       " GaussianNB(priors=None, var_smoothing=1e-09),\n",
       " MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False),\n",
       " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                      metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "                      weights='uniform'),\n",
       " NuSVC(break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "       decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "       max_iter=-1, nu=0.5, probability=True, random_state=100, shrinking=True,\n",
       "       tol=0.001, verbose=False),\n",
       " SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "     max_iter=-1, probability=True, random_state=100, shrinking=True, tol=0.001,\n",
       "     verbose=False),\n",
       " DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=100, splitter='best'),\n",
       " ExtraTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=1, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, random_state=100,\n",
       "                     splitter='random')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adaboost': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'bagging': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'extratrees': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'gradientboosting': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'randomforest': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'xgboost': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'logreg': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'bernoulli': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'gaussian': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'multinomial': AttributeError(\"'MultinomialNB' object has no attribute 'feature_log_prob_'\"),\n",
       " 'complement': AttributeError(\"'ComplementNB' object has no attribute 'feature_log_prob_'\"),\n",
       " 'knn': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'nu': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'svc': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'decision': array([2, 0, 0, ..., 1, 1, 1]),\n",
       " 'extra': array([2, 0, 0, ..., 1, 1, 1])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = models.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adaboost': array([[0.34227098, 0.28903583, 0.36869319],\n",
       "        [0.35026398, 0.32208636, 0.32764966],\n",
       "        [0.35432213, 0.32600699, 0.31967088],\n",
       "        ...,\n",
       "        [0.31837592, 0.37471306, 0.30691102],\n",
       "        [0.31837592, 0.37471306, 0.30691102],\n",
       "        [0.31893436, 0.35985944, 0.3212062 ]]),\n",
       " 'bagging': array([[0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]]),\n",
       " 'extratrees': array([[0.  , 0.  , 1.  ],\n",
       "        [0.97, 0.  , 0.03],\n",
       "        [1.  , 0.  , 0.  ],\n",
       "        ...,\n",
       "        [0.  , 1.  , 0.  ],\n",
       "        [0.  , 1.  , 0.  ],\n",
       "        [0.  , 1.  , 0.  ]]),\n",
       " 'gradientboosting': array([[0.00376862, 0.00122177, 0.99500961],\n",
       "        [0.9857674 , 0.00364433, 0.01058827],\n",
       "        [0.98765278, 0.00348545, 0.00886177],\n",
       "        ...,\n",
       "        [0.0015049 , 0.99731264, 0.00118245],\n",
       "        [0.0016525 , 0.99720732, 0.00114018],\n",
       "        [0.00281662, 0.99520107, 0.00198231]]),\n",
       " 'randomforest': array([[0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]]),\n",
       " 'xgboost': array([[2.3536861e-04, 5.8299560e-05, 9.9970633e-01],\n",
       "        [9.9550062e-01, 1.9797798e-04, 4.3013538e-03],\n",
       "        [9.9890316e-01, 4.8600332e-04, 6.1081146e-04],\n",
       "        ...,\n",
       "        [2.9073481e-04, 9.9962795e-01, 8.1310878e-05],\n",
       "        [4.0018518e-04, 9.9955708e-01, 4.2772928e-05],\n",
       "        [1.4229633e-04, 9.9973625e-01, 1.2148532e-04]], dtype=float32),\n",
       " 'logreg': array([[2.93446262e-03, 2.64662887e-09, 9.97065535e-01],\n",
       "        [9.36438001e-01, 1.41348234e-03, 6.21485163e-02],\n",
       "        [9.58958087e-01, 7.69029353e-03, 3.33516190e-02],\n",
       "        ...,\n",
       "        [9.56087581e-04, 9.99043616e-01, 2.96740218e-07],\n",
       "        [8.89746871e-04, 9.99110028e-01, 2.24659822e-07],\n",
       "        [3.55754370e-02, 9.64309144e-01, 1.15419347e-04]]),\n",
       " 'bernoulli': array([[2.06050644e-02, 9.27665077e-08, 9.79394843e-01],\n",
       "        [9.78438872e-01, 8.02467075e-05, 2.14808815e-02],\n",
       "        [9.79156462e-01, 7.26743460e-05, 2.07708640e-02],\n",
       "        ...,\n",
       "        [2.12256425e-02, 9.78764870e-01, 9.48758649e-06],\n",
       "        [2.34022799e-02, 9.76587613e-01, 1.01073461e-05],\n",
       "        [2.12256425e-02, 9.78764870e-01, 9.48758649e-06]]),\n",
       " 'gaussian': array([[2.05852480e-03, 3.41530184e-26, 9.97941475e-01],\n",
       "        [9.99994342e-01, 3.86880631e-24, 5.65765739e-06],\n",
       "        [9.99973654e-01, 1.76814681e-17, 2.63457911e-05],\n",
       "        ...,\n",
       "        [1.46109423e-04, 9.99853891e-01, 1.38961988e-11],\n",
       "        [1.45262656e-04, 9.99854737e-01, 8.23883982e-12],\n",
       "        [2.48348236e-03, 9.97516408e-01, 1.09524875e-07]]),\n",
       " 'multinomial': AttributeError(\"'MultinomialNB' object has no attribute 'feature_log_prob_'\"),\n",
       " 'complement': AttributeError(\"'ComplementNB' object has no attribute 'feature_log_prob_'\"),\n",
       " 'knn': array([[0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]]),\n",
       " 'nu': array([[1.00824433e-06, 5.03483293e-07, 9.99998488e-01],\n",
       "        [9.87461198e-01, 1.75279169e-04, 1.23635229e-02],\n",
       "        [9.90054392e-01, 2.39903102e-04, 9.70570529e-03],\n",
       "        ...,\n",
       "        [1.23621533e-06, 9.99471827e-01, 5.26936656e-04],\n",
       "        [3.66217428e-03, 9.92790941e-01, 3.54688500e-03],\n",
       "        [6.53997294e-03, 9.91038294e-01, 2.42173315e-03]]),\n",
       " 'svc': array([[2.92458182e-06, 1.46668854e-06, 9.99995609e-01],\n",
       "        [9.73848119e-01, 8.06562978e-03, 1.80862516e-02],\n",
       "        [9.82168968e-01, 3.78409457e-03, 1.40469371e-02],\n",
       "        ...,\n",
       "        [6.34210019e-07, 9.99143406e-01, 8.55959989e-04],\n",
       "        [7.80409884e-06, 9.97237965e-01, 2.75423129e-03],\n",
       "        [5.16251595e-03, 9.84059553e-01, 1.07779313e-02]]),\n",
       " 'decision': array([[0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]]),\n",
       " 'extra': array([[0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probas = models.predict_proba(X_test)\n",
    "pred_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.evaluate(test_X=X_test, test_Y=y_test, idx_label_dic=None, class_report=\"classf_report.csv\", con_mat=\"confusion_matrix.csv\", pred_proba=\"predictions_proba.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
